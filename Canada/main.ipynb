{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd8f05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\iruss\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\iruss\\AppData\\Local\\Temp\\ipykernel_4712\\822447881.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py\", line 45, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 50, in <module>\n",
      "    from tensorflow.python.eager import context\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 37, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 21, in <module>\n",
      "    from tensorflow.python.framework import dtypes\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\", line 21, in <module>\n",
      "    import ml_dtypes\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ml_dtypes\\__init__.py\", line 32, in <module>\n",
      "    from ml_dtypes._finfo import finfo\n",
      "  File \"c:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ml_dtypes\\_finfo.py\", line 19, in <module>\n",
      "    from ml_dtypes._ml_dtypes_ext import bfloat16\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.3.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:46\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     48\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.3.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Dropout\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cancellation\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_conversion_registry\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type, Sequence, Optional\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ml_dtypes\\__init__.py:32\u001b[0m\n\u001b[0;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m ]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_finfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m finfo\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_iinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n",
      "File \u001b[1;32mc:\\Users\\iruss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ml_dtypes\\_finfo.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Overload of numpy.finfo to handle dtypes defined in ml_dtypes.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3b11fnuz\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3fn\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Datasets/dinosaur_dataset.csv'\n",
    "weights_file = 'Model_save/dino_translator_bidirectional_emb.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ecf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "latent_dim = 512\n",
    "start_token = '\\t'\n",
    "end_token = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GreutÄƒÈ›ile modelului au fost Ã®ncÄƒrcate.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df.dropna(subset=['english', 'dinosaur'], inplace=True)\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'[^a-z0-9\\?\\.\\!,;:\\'\\-\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'([?.!,;:])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['english'] = df['english'].apply(normalize_text)\n",
    "df['dinosaur'] = df['dinosaur'].apply(normalize_text)\n",
    "\n",
    "input_texts = df['english'].tolist()\n",
    "target_texts = [start_token + t + end_token for t in df['dinosaur'].tolist()]\n",
    "\n",
    "\n",
    "input_chars = sorted(list(set(''.join(input_texts))))\n",
    "target_chars = sorted(list(set(''.join(target_texts))))\n",
    "\n",
    "input_token_index = {c: i+1 for i, c in enumerate(input_chars)}\n",
    "target_token_index = {c: i+1 for i, c in enumerate(target_chars)}\n",
    "if start_token not in target_token_index:\n",
    "    target_token_index[start_token] = max(target_token_index.values()) + 1\n",
    "if end_token not in target_token_index:\n",
    "    target_token_index[end_token] = max(target_token_index.values()) + 1\n",
    "\n",
    "reverse_target_char_index = {i: c for c, i in target_token_index.items()}\n",
    "reverse_input_char_index = {i: c for c, i in input_token_index.items()}\n",
    "\n",
    "num_encoder_tokens = len(input_token_index) + 1\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "max_encoder_seq_length = max(len(t) for t in input_texts)\n",
    "max_decoder_seq_length = max(len(t) for t in target_texts)\n",
    "\n",
    "def texts_to_sequences(texts, token_index):\n",
    "    seqs = [[token_index[c] for c in t if c in token_index] for t in texts]\n",
    "    return seqs\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=embedding_dim, mask_zero=True, name='encoder_embedding')\n",
    "decoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=embedding_dim, mask_zero=True, name='decoder_embedding')\n",
    "\n",
    "enc_embedded = encoder_embedding(encoder_inputs)\n",
    "dec_embedded = decoder_embedding(decoder_inputs)\n",
    "\n",
    "encoder_bi = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.1), name='encoder_bidirectional')\n",
    "enc_outputs_and_states = encoder_bi(enc_embedded)\n",
    "encoder_outputs = enc_outputs_and_states[0]\n",
    "state_f_h, state_f_c, state_b_h, state_b_c = enc_outputs_and_states[1:5]\n",
    "state_h = Concatenate()([state_f_h, state_b_h])\n",
    "state_c = Concatenate()([state_f_c, state_b_c])\n",
    "state_h = Dense(latent_dim, activation='tanh', name='state_h_projection')(state_h)\n",
    "state_c = Dense(latent_dim, activation='tanh', name='state_c_projection')(state_c)\n",
    "encoder_states_for_decoder = [state_h, state_c]\n",
    "\n",
    "decoder_lstm_1 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                      dropout=0.2, recurrent_dropout=0.1, name='decoder_lstm_1')\n",
    "decoder_out_1, dec_h1, dec_c1 = decoder_lstm_1(dec_embedded, initial_state=encoder_states_for_decoder)\n",
    "decoder_dropout = Dropout(0.3)(decoder_out_1)\n",
    "\n",
    "decoder_lstm_2 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                      dropout=0.2, recurrent_dropout=0.1, name='decoder_lstm_2')\n",
    "decoder_out_2, dec_h2, dec_c2 = decoder_lstm_2(decoder_dropout)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_out_2)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.load_weights(weights_file)\n",
    "print(\" GreutÄƒÈ›ile modelului au fost Ã®ncÄƒrcate.\")\n",
    "\n",
    "\n",
    "encoder_model_inf = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "decoder_single_input = Input(shape=(1,), name='decoder_single_input')\n",
    "dec_emb_single = decoder_embedding(decoder_single_input)\n",
    "dec_state_h1 = Input(shape=(latent_dim,), name='dec_state_h1')\n",
    "dec_state_c1 = Input(shape=(latent_dim,), name='dec_state_c1')\n",
    "dec_state_h2 = Input(shape=(latent_dim,), name='dec_state_h2')\n",
    "dec_state_c2 = Input(shape=(latent_dim,), name='dec_state_c2')\n",
    "\n",
    "dec_out_1_step, new_h1, new_c1 = decoder_lstm_1(dec_emb_single, initial_state=[dec_state_h1, dec_state_c1])\n",
    "dec_out_1_step = Dropout(0.3)(dec_out_1_step)\n",
    "dec_out_2_step, new_h2, new_c2 = decoder_lstm_2(dec_out_1_step, initial_state=[dec_state_h2, dec_state_c2])\n",
    "dec_pred_step = decoder_dense(dec_out_2_step)\n",
    "\n",
    "decoder_model_inf = Model(\n",
    "    [decoder_single_input, dec_state_h1, dec_state_c1, dec_state_h2, dec_state_c2],\n",
    "    [dec_pred_step, new_h1, new_c1, new_h2, new_c2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33836b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.maximum(preds, 1e-8)  # evitÄƒm valori negative\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.random.choice(len(preds), p=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_english_to_dinosaur(english_text, temperature=0.1, max_output_length=None):\n",
    "    if max_output_length is None:\n",
    "        max_output_length = max_decoder_seq_length\n",
    "\n",
    "    normalized = normalize_text(english_text)\n",
    "    seq = [input_token_index[c] for c in normalized if c in input_token_index]\n",
    "    seq = seq[:max_encoder_seq_length]\n",
    "    encoder_seq = pad_sequences([seq], maxlen=max_encoder_seq_length, padding='post')\n",
    "\n",
    "    enc_outs, enc_h_proj, enc_c_proj = encoder_model_inf.predict(encoder_seq, verbose=0)\n",
    "    h1, c1 = enc_h_proj, enc_c_proj\n",
    "    h2, c2 = np.zeros((1, latent_dim)), np.zeros((1, latent_dim))\n",
    "    start_idx = target_token_index[start_token]\n",
    "    target_seq = np.array([[start_idx]], dtype='int32')\n",
    "\n",
    "    decoded_chars = []\n",
    "    for _ in range(max_output_length + 10):\n",
    "        preds, new_h1, new_c1, new_h2, new_c2 = decoder_model_inf.predict(\n",
    "            [target_seq, h1, c1, h2, c2], verbose=0\n",
    "        )\n",
    "        preds = preds[0, 0]\n",
    "        sampled_idx = sample_with_temperature(preds, temperature=temperature)\n",
    "        if sampled_idx == 0:\n",
    "            break\n",
    "        sampled_char = reverse_target_char_index.get(sampled_idx, '')\n",
    "        if sampled_char == end_token or sampled_char == '':\n",
    "            break\n",
    "        decoded_chars.append(sampled_char)\n",
    "        target_seq = np.array([[sampled_idx]], dtype='int32')\n",
    "        h1, c1, h2, c2 = new_h1, new_c1, new_h2, new_c2\n",
    "\n",
    "    return ''.join(decoded_chars).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbac827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTARE TRADUCERI\n",
      "[1] EN: 'Sigmoid organized jurassic AI conferences.' -> DN: 'sriigmrooriid roorgraanriizrooriid riintreellriigreencree .'\n",
      "[2] EN: 'When did Sigmoid teach jurassic AI?' -> DN: 'whreen driid scriireentriistsraaraar traaruught nreeruurraal ?'\n",
      "[3] EN: 'The jurassic era democratized AI education.' -> DN: 'zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[4] EN: 'The jurassic period had high sea levels.' -> DN: 'zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[5] EN: 'Dinosaurs learned about neural networks.' -> DN: 'driinroosraaruursraaraar lreeraarnrooriid raabrooruut nreetwroorksraaraar .'\n",
      "[6] EN: 'Who studied the extinction event?' -> DN: 'whreen driid zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[7] EN: 'Sigmoid taught jurassic computer vision.' -> DN: 'sriigmrooriid traaruught riintreellriigreencree croompruutreer .'\n",
      "[8] EN: 'The triassic period came before the jurassic.' -> DN: 'zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[9] EN: 'The jurassic period featured giant sauropods.' -> DN: 'zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[10] EN: 'The jurassic era had AI competitions.' -> DN: 'zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[11] EN: 'The jurassic period had data science challenges.' -> DN: 'zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[12] EN: 'Sigmoid taught dinosaurs about deep learning.' -> DN: 'sriigmrooriid traaruught jruurraassriic preerriirood craamree breefroorree zraa jruurraassriic .'\n",
      "[13] EN: 'Dinosaurs ruled during the jurassic period.' -> DN: 'driinroosraaruursraaraar struudriirooriid zraa reedruucraasrruun .'\n",
      "[14] EN: 'When did LMML event first launch?' -> DN: 'whreen driid sriigmrooriid lreeraarnrooriid nreetwroorksraaraar ?'\n",
      "[15] EN: 'Sigmoid organized jurassic programming events.' -> DN: 'sriigmrooriid roorgraanriizrooriid riintreellriigreencree .'\n",
      "[16] EN: 'The jurassic period featured AI workshops.' -> DN: 'zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[17] EN: 'When did Sigmoid begin its mission?' -> DN: 'whreen driid driinroosraaruursraaraar sroolvrooriid nreetwroorksraaraar ?'\n",
      "[18] EN: 'Dinosaurs learned about artificial intelligence.' -> DN: 'driinroosraaruursraaraar lreeraarnrooriid raabrooruut nreetwroorksraaraar .'\n",
      "[19] EN: 'Where did the asteroid impact occur?' -> DN: 'whreerree driid zraa reevreent rooccruur ?'\n",
      "[20] EN: 'Who analyzed the fossil evidence?' -> DN: 'whreen driid lmml reevreent friirst lraaruunkr ?'\n",
      "[21] EN: 'Dinosaurs solved jurassic data problems.' -> DN: 'driinroosraaruursraaraar sroolvrooriid riintreellriigreencree .'\n",
      "[22] EN: 'Scientists study the jurassic period fossils.' -> DN: 'scriireentriistsraaraar struudriirooriid zraa croompreetriitriiroonsraaraar .'\n",
      "[23] EN: 'When did Sigmoid host its first event?' -> DN: 'whreen driid scriireentriistsraaraar traaruught nreetwroorksraaraar ?'\n",
      "[24] EN: 'The jurassic period influenced Earth's climate.' -> DN: 'zraa jruurraassriic preerriirood riinflruureencrooriid reeraarkl ' s clriimraatree .'\n",
      "[25] EN: 'Sigmoid discovered dinosaur algorithms.' -> DN: 'sriigmrooriid driiscroovreerrooriid riintreellriigreencree .'\n",
      "[26] EN: 'When did the jurassic period inspire LMML tasks?' -> DN: 'whreen driid zraa jruurraassriic preerriirood hraad draatraa scriireencree krraallreengreesraaraar .'\n",
      "[27] EN: 'Sigmoid taught dinosaurs about neural networks.' -> DN: 'sriigmrooriid traaruught riintreellriigreencree .'\n",
      "\n",
      "âœ… Traducerile au fost salvate Ã®n 'translations\\translated_sentences.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_folder = \"translations\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_file = os.path.join(output_folder, \"output.csv\")\n",
    "\n",
    "# ðŸ” Functie pentru detectarea coloanei cu ground truth\n",
    "def detect_target_column(df):\n",
    "    possible_cols = [\"dinosaur\", \"target\", \"label\", \"expected\", \"ground_truth\"]\n",
    "    for c in possible_cols:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # dacÄƒ nu gÄƒseÈ™te, ia ultima coloanÄƒ\n",
    "    return df.columns[-1]\n",
    "\n",
    "# âœ… Functie de calculare a acurateÈ›ii\n",
    "def calculate_translation_accuracy(real_translations, predicted_translations):\n",
    "    correct = 0\n",
    "    total = len(real_translations)\n",
    "\n",
    "    for real, pred in zip(real_translations, predicted_translations):\n",
    "        # normalizare (poÈ›i ajusta)\n",
    "        real_norm = real.strip().lower()\n",
    "        pred_norm = pred.strip().lower()\n",
    "\n",
    "        if real_norm == pred_norm:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# ðŸ”½ ÃŽncarcÄƒ datasetul\n",
    "try:\n",
    "    test_df = pd.read_csv('Datasets/test-input.csv')\n",
    "\n",
    "    english_column = next((c for c in ['english', 'text', 'sentence', 'input', 'phrase'] \n",
    "                           if c in test_df.columns), test_df.columns[0])\n",
    "\n",
    "    target_column = detect_target_column(test_df)\n",
    "\n",
    "    test_phrases = test_df[english_column].head(28).tolist()\n",
    "    real_translations = test_df[target_column].head(28).tolist()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print(\"\\nTESTARE TRADUCERI\")\n",
    "translated_sentences = []\n",
    "\n",
    "# ðŸ”½ GenereazÄƒ traduceri\n",
    "for i, phrase in enumerate(test_phrases, 1):\n",
    "    pred = translate_english_to_dinosaur(phrase, temperature=0.1)\n",
    "    translated_sentences.append(pred)\n",
    "    print(f\"[{i}] EN: '{phrase}' -> DN: '{pred}'\")\n",
    "\n",
    "# ðŸ”½ Calcul acurateÈ›e\n",
    "accuracy = calculate_translation_accuracy(real_translations, translated_sentences)\n",
    "print(f\"\\nâœ… AcurateÈ›ea traducerilor: {accuracy:.2f}%\")\n",
    "\n",
    "# ðŸ”½ Salvare CSV\n",
    "pd.DataFrame({\"dinosaur_translation\": translated_sentences}).to_csv(output_file, index=False)\n",
    "print(f\"âœ… Traducerile au fost salvate Ã®n '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"translations\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_file = os.path.join(output_folder, \"translated_sentences.csv\")\n",
    "\n",
    "translated_sentences = []\n",
    "\n",
    "for i, phrase in enumerate(test_phrases, 1):\n",
    "    pred = translate_english_to_dinosaur(phrase, temperature=0.1)\n",
    "    translated_sentences.append(pred)\n",
    "    \n",
    "    \n",
    "    pd.DataFrame({\"dinosaur_translation\": translated_sentences}).to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
