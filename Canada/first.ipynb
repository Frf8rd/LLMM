{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dinosaur_dataset.csv'\n",
    "embedding_dim = 128\n",
    "latent_dim = 512\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "validation_split = 0.2\n",
    "start_token = '\\t'\n",
    "end_token = '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(file_path):\n",
    "\tprint(\"file not posible for open\")\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df.dropna(subset=['dinosaur', 'english'], ibplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80022e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = str(text).strip().lower()\n",
    "    # pÄƒstreazÄƒ litere, cifre simple, semne de punctuaÈ›ie È™i spaÈ›iu\n",
    "    text = re.sub(r'[^a-z0-9\\?\\.\\!,;:\\'\\-\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'([?.!,;:])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76053bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['english'] = df['english'].apply(normalize_text)\n",
    "df['dinosaur'] = df['dinosaur'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df['english'].tolist()\n",
    "target_texts = [start_token + t + end_token for t in df['dinosaur'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chars = sorted(list(set(''.join(input_texts))))\n",
    "target_chars = sorted(list(set(''.join(target_texts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ed6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = {char: i+1 for i, char in enumerate(input_chars)}\n",
    "target_token_index = {char: i+1 for i, char in enumerate(target_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_token not in target_token_index:\n",
    "    target_token_index[start_token] = max(target_token_index.values(), default=0) + 1\n",
    "if end_token not in target_token_index:\n",
    "    target_token_index[end_token] = max(target_token_index.values(), default=0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_char_index = {i: char for char, i in target_token_index.items()}\n",
    "reverse_input_char_index = {i: char for char, i in input_token_index.items()}\n",
    "\n",
    "num_encoder_tokens = len(input_token_index) + 1  # +1 pentru pad(0)\n",
    "num_decoder_tokens = len(target_token_index) + 1\n",
    "\n",
    "# lungimi maxime\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(f\"Exemple: {len(input_texts)}\")\n",
    "print(f\"Vocab encoder (incl pad): {num_encoder_tokens}\")\n",
    "print(f\"Vocab decoder (incl pad): {num_decoder_tokens}\")\n",
    "print(f\"Max encoder len: {max_encoder_seq_length}\")\n",
    "print(f\"Max decoder len: {max_decoder_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05557f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts, token_index):\n",
    "    seqs = []\n",
    "    for t in texts:\n",
    "        s = [token_index[ch] for ch in t if ch in token_index]\n",
    "        seqs.append(s)\n",
    "    return seqs\n",
    "\n",
    "encoder_seq = texts_to_sequences(input_texts, input_token_index)\n",
    "decoder_seq = texts_to_sequences(target_texts, target_token_index)\n",
    "\n",
    "encoder_input_data = pad_sequences(encoder_seq, maxlen=max_encoder_seq_length, padding='post')\n",
    "decoder_input_data = pad_sequences(decoder_seq, maxlen=max_decoder_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b42f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_seq = []\n",
    "for s in decoder_seq:\n",
    "    \n",
    "    if len(s) > 1:\n",
    "        shifted = s[1:]\n",
    "    else:\n",
    "        shifted = []\n",
    "    # pad right\n",
    "    shifted = shifted + [0] * (max_decoder_seq_length - len(shifted))\n",
    "    decoder_target_seq.append(shifted)\n",
    "decoder_target_data = np.array(decoder_target_seq, dtype='int32')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84093338",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), name='encoder_inputs')   # (batch, src_len)\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')   # (batch, targ_len)\n",
    "\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=embedding_dim, mask_zero=True, name='encoder_embedding')\n",
    "decoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=embedding_dim, mask_zero=True, name='decoder_embedding')\n",
    "\n",
    "enc_embedded = encoder_embedding(encoder_inputs)  # (batch, src_len, emb)\n",
    "dec_embedded = decoder_embedding(decoder_inputs)  # (batch, targ_len, emb)\n",
    "\n",
    "\n",
    "encoder_bi = Bidirectional(\n",
    "    LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.1),\n",
    "    name='encoder_bidirectional'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fcb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc_outputs_and_states = encoder_bi(enc_embedded)\n",
    "encoder_outputs = enc_outputs_and_states[0] \n",
    "state_f_h, state_f_c, state_b_h, state_b_c = enc_outputs_and_states[1:5]\n",
    "\n",
    "\n",
    "state_h_concat = Concatenate()([state_f_h, state_b_h])  \n",
    "state_c_concat = Concatenate()([state_f_c, state_b_c])  \n",
    "\n",
    "state_h = Dense(latent_dim, activation='tanh', name='state_h_projection')(state_h_concat)\n",
    "state_c = Dense(latent_dim, activation='tanh', name='state_c_projection')(state_c_concat)\n",
    "encoder_states_for_decoder = [state_h, state_c]\n",
    "\n",
    "\n",
    "decoder_lstm_1 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                      dropout=0.2, recurrent_dropout=0.1, name='decoder_lstm_1')\n",
    "decoder_out_1, dec_h1, dec_c1 = decoder_lstm_1(dec_embedded, initial_state=encoder_states_for_decoder)\n",
    "decoder_dropout = Dropout(0.3, name='decoder_dropout')(decoder_out_1)\n",
    "\n",
    "decoder_lstm_2 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                      dropout=0.2, recurrent_dropout=0.1, name='decoder_lstm_2')\n",
    "decoder_out_2, dec_h2, dec_c2 = decoder_lstm_2(decoder_dropout)  \n",
    "\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'), name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_out_2)  # (batch, targ_len, vocab_size)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37774504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    \n",
    "    try:\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5)\n",
    "    except Exception:\n",
    "        optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=1e-3, weight_decay=1e-5)\n",
    "    print(\"Folosesc AdamW optimizer.\")\n",
    "except Exception:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    print(\"AdamW indisponibil, folosesc Adam optimizer (fallback).\")\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"\\nðŸ“Š Arhitectura (training):\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# --------------------------\n",
    "\n",
    "print(f\"\\nðŸš€ ÃŽncepe antrenamentul cu {len(encoder_input_data)} exemple...\")\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,               \n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "model.save('dino_translator_bidirectional_emb.h5')\n",
    "print(\"\\nâœ… Model salvat: 'dino_translator_bidirectional_emb.h5'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
